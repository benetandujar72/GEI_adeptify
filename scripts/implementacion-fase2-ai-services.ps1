# Script para implementar FASE 2 - SERVICIOS AI
# EduAI Platform - Servicios de Inteligencia Artificial

Write-Host "ü§ñ IMPLEMENTANDO FASE 2 - SERVICIOS AI..." -ForegroundColor Green
Write-Host "üìã LLM Gateway, Content Generation, Chatbot, Predictive Analytics, Personalization Engine, ML Pipeline" -ForegroundColor Cyan
Write-Host ""

# 1. IMPLEMENTAR LLM GATEWAY COMPLETO
Write-Host "üß† Implementando LLM Gateway completo..." -ForegroundColor Yellow

# Package.json para LLM Gateway
$llmGatewayPackageJson = @"
{
  "name": "llm-gateway",
  "version": "1.0.0",
  "description": "EduAI LLM Gateway Service",
  "main": "dist/index.js",
  "scripts": {
    "build": "esbuild src/index.ts --bundle --platform=node --target=node18 --outfile=dist/index.js",
    "start": "node dist/index.js",
    "dev": "tsx src/index.ts",
    "test": "jest",
    "lint": "eslint src/**/*.ts"
  },
  "dependencies": {
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "helmet": "^7.0.0",
    "dotenv": "^16.3.1",
    "redis": "^4.6.8",
    "jsonwebtoken": "^9.0.2",
    "winston": "^3.10.0",
    "express-rate-limit": "^6.10.0",
    "express-validator": "^7.0.1",
    "@anthropic-ai/sdk": "^0.8.1",
    "@google/generative-ai": "^0.1.3",
    "openai": "^4.20.1",
    "node-cache": "^5.1.2",
    "moment": "^2.29.4",
    "lodash": "^4.17.21"
  },
  "devDependencies": {
    "@types/express": "^4.17.17",
    "@types/cors": "^2.8.13",
    "@types/node": "^20.5.0",
    "@types/jsonwebtoken": "^9.0.2",
    "@types/lodash": "^4.14.198",
    "typescript": "^5.1.6",
    "tsx": "^3.12.7",
    "esbuild": "^0.18.17",
    "eslint": "^8.47.0",
    "@typescript-eslint/eslint-plugin": "^6.4.1",
    "@typescript-eslint/parser": "^6.4.1",
    "jest": "^29.6.2",
    "@types/jest": "^29.5.4",
    "ts-jest": "^29.1.1"
  }
}
"@

Set-Content -Path "microservices/llm-gateway/package.json" -Value $llmGatewayPackageJson

# LLM Gateway Service
$llmGatewayService = @"
import { createClient } from 'redis';
import winston from 'winston';
import { Anthropic } from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';
import OpenAI from 'openai';
import NodeCache from 'node-cache';
import _ from 'lodash';

// Configurar logger
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// Configurar Redis
const redisClient = createClient({
  url: process.env.REDIS_URL || 'redis://localhost:6379'
});

// Configurar cache
const cache = new NodeCache({ stdTTL: 3600 }); // 1 hora

// Configurar proveedores de LLM
const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY || '');

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Interfaces
interface LLMRequest {
  provider: 'anthropic' | 'google' | 'openai';
  model: string;
  prompt: string;
  maxTokens?: number;
  temperature?: number;
  userId?: string;
  sessionId?: string;
}

interface LLMResponse {
  id: string;
  content: string;
  provider: string;
  model: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  processingTime: number;
  cached: boolean;
  timestamp: Date;
}

interface ProviderConfig {
  name: string;
  models: string[];
  maxTokens: number;
  temperature: number;
}

class LLMGatewayService {
  private providers: Map<string, ProviderConfig> = new Map();
  private requestCount = 0;
  private errorCount = 0;

  constructor() {
    this.initializeProviders();
  }

  private initializeProviders() {
    this.providers.set('anthropic', {
      name: 'Anthropic Claude',
      models: ['claude-3-sonnet-20240229', 'claude-3-haiku-20240307', 'claude-3-opus-20240229'],
      maxTokens: 4096,
      temperature: 0.7
    });

    this.providers.set('google', {
      name: 'Google Gemini',
      models: ['gemini-pro', 'gemini-pro-vision'],
      maxTokens: 2048,
      temperature: 0.7
    });

    this.providers.set('openai', {
      name: 'OpenAI GPT',
      models: ['gpt-4', 'gpt-3.5-turbo'],
      maxTokens: 4096,
      temperature: 0.7
    });
  }

  async processRequest(request: LLMRequest): Promise<LLMResponse> {
    const startTime = Date.now();
    const requestId = this.generateRequestId();
    
    try {
      // Verificar cache
      const cacheKey = this.generateCacheKey(request);
      const cachedResponse = cache.get(cacheKey);
      
      if (cachedResponse) {
        logger.info(`Cache hit for request: ${requestId}`);
        return {
          ...cachedResponse,
          id: requestId,
          cached: true,
          timestamp: new Date()
        } as LLMResponse;
      }

      // Procesar request
      let response: LLMResponse;
      
      switch (request.provider) {
        case 'anthropic':
          response = await this.processAnthropicRequest(request, requestId);
          break;
        case 'google':
          response = await this.processGoogleRequest(request, requestId);
          break;
        case 'openai':
          response = await this.processOpenAIRequest(request, requestId);
          break;
        default:
          throw new Error(`Unsupported provider: ${request.provider}`);
      }

      response.processingTime = Date.now() - startTime;
      response.cached = false;
      response.timestamp = new Date();

      // Guardar en cache
      cache.set(cacheKey, response);

      // Actualizar m√©tricas
      this.requestCount++;
      await this.updateMetrics(request.provider, response.processingTime);

      logger.info(`Processed LLM request: ${requestId}, provider: ${request.provider}, time: ${response.processingTime}ms`);
      
      return response;

    } catch (error) {
      this.errorCount++;
      logger.error(`Failed to process LLM request: ${requestId}`, error);
      throw error;
    }
  }

  private async processAnthropicRequest(request: LLMRequest, requestId: string): Promise<LLMResponse> {
    const model = request.model || 'claude-3-sonnet-20240229';
    const maxTokens = request.maxTokens || 4096;
    const temperature = request.temperature || 0.7;

    const response = await anthropic.messages.create({
      model,
      max_tokens: maxTokens,
      temperature,
      messages: [{ role: 'user', content: request.prompt }]
    });

    return {
      id: requestId,
      content: response.content[0].text,
      provider: 'anthropic',
      model,
      usage: {
        promptTokens: response.usage.input_tokens,
        completionTokens: response.usage.output_tokens,
        totalTokens: response.usage.input_tokens + response.usage.output_tokens
      },
      processingTime: 0,
      cached: false,
      timestamp: new Date()
    };
  }

  private async processGoogleRequest(request: LLMRequest, requestId: string): Promise<LLMResponse> {
    const model = request.model || 'gemini-pro';
    const maxTokens = request.maxTokens || 2048;
    const temperature = request.temperature || 0.7;

    const geminiModel = genAI.getGenerativeModel({ model });
    
    const result = await geminiModel.generateContent({
      contents: [{ role: 'user', parts: [{ text: request.prompt }] }],
      generationConfig: {
        maxOutputTokens: maxTokens,
        temperature
      }
    });

    const response = await result.response;
    const text = response.text();

    return {
      id: requestId,
      content: text,
      provider: 'google',
      model,
      usage: {
        promptTokens: 0, // Google no proporciona tokens exactos
        completionTokens: text.length,
        totalTokens: text.length
      },
      processingTime: 0,
      cached: false,
      timestamp: new Date()
    };
  }

  private async processOpenAIRequest(request: LLMRequest, requestId: string): Promise<LLMResponse> {
    const model = request.model || 'gpt-3.5-turbo';
    const maxTokens = request.maxTokens || 4096;
    const temperature = request.temperature || 0.7;

    const response = await openai.chat.completions.create({
      model,
      max_tokens: maxTokens,
      temperature,
      messages: [{ role: 'user', content: request.prompt }]
    });

    return {
      id: requestId,
      content: response.choices[0].message.content || '',
      provider: 'openai',
      model,
      usage: {
        promptTokens: response.usage.prompt_tokens,
        completionTokens: response.usage.completion_tokens,
        totalTokens: response.usage.total_tokens
      },
      processingTime: 0,
      cached: false,
      timestamp: new Date()
    };
  }

  private generateRequestId(): string {
    return `llm_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private generateCacheKey(request: LLMRequest): string {
    return `llm:${request.provider}:${request.model}:${_.hash(request.prompt)}`;
  }

  private async updateMetrics(provider: string, processingTime: number): Promise<void> {
    const metricsKey = `llm:metrics:${provider}`;
    const metrics = await redisClient.get(metricsKey);
    
    const currentMetrics = metrics ? JSON.parse(metrics) : {
      requestCount: 0,
      totalProcessingTime: 0,
      averageProcessingTime: 0,
      lastUpdated: new Date().toISOString()
    };

    currentMetrics.requestCount++;
    currentMetrics.totalProcessingTime += processingTime;
    currentMetrics.averageProcessingTime = currentMetrics.totalProcessingTime / currentMetrics.requestCount;
    currentMetrics.lastUpdated = new Date().toISOString();

    await redisClient.setEx(metricsKey, 86400, JSON.stringify(currentMetrics)); // 24 horas TTL
  }

  async getHealthStatus(): Promise<Record<string, any>> {
    return {
      status: 'healthy',
      service: 'llm-gateway',
      providers: Array.from(this.providers.keys()),
      metrics: {
        requestCount: this.requestCount,
        errorCount: this.errorCount,
        errorRate: this.requestCount > 0 ? (this.errorCount / this.requestCount) * 100 : 0
      },
      cache: {
        size: cache.keys().length,
        hits: cache.getStats().hits,
        misses: cache.getStats().misses
      },
      timestamp: new Date().toISOString()
    };
  }

  async getProviderStatus(): Promise<Record<string, any>> {
    const status: Record<string, any> = {};
    
    for (const [provider, config] of this.providers) {
      const metricsKey = `llm:metrics:${provider}`;
      const metrics = await redisClient.get(metricsKey);
      
      status[provider] = {
        name: config.name,
        models: config.models,
        metrics: metrics ? JSON.parse(metrics) : {}
      };
    }

    return status;
  }

  async clearCache(): Promise<void> {
    cache.flushAll();
    logger.info('LLM Gateway cache cleared');
  }
}

export default new LLMGatewayService();
"@

Set-Content -Path "microservices/llm-gateway/src/services/llm-gateway.ts" -Value $llmGatewayService

# 2. IMPLEMENTAR CONTENT GENERATION SERVICE
Write-Host "üìù Implementando Content Generation Service..." -ForegroundColor Yellow

$contentGenerationService = @"
import { createClient } from 'redis';
import winston from 'winston';
import { Anthropic } from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';
import NodeCache from 'node-cache';
import _ from 'lodash';

// Configurar logger
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// Configurar Redis
const redisClient = createClient({
  url: process.env.REDIS_URL || 'redis://localhost:6379'
});

// Configurar cache
const cache = new NodeCache({ stdTTL: 7200 }); // 2 horas

// Configurar proveedores
const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY || '');

// Interfaces
interface ContentRequest {
  type: 'lesson' | 'quiz' | 'assignment' | 'summary' | 'explanation' | 'story';
  subject: string;
  grade: string;
  topic: string;
  length: 'short' | 'medium' | 'long';
  style: 'formal' | 'casual' | 'creative' | 'technical';
  language: string;
  userId?: string;
}

interface ContentResponse {
  id: string;
  type: string;
  title: string;
  content: string;
  metadata: {
    subject: string;
    grade: string;
    topic: string;
    difficulty: string;
    estimatedTime: number;
    learningObjectives: string[];
    keywords: string[];
  };
  processingTime: number;
  cached: boolean;
  timestamp: Date;
}

interface ContentTemplate {
  type: string;
  prompt: string;
  structure: string[];
}

class ContentGenerationService {
  private templates: Map<string, ContentTemplate> = new Map();
  private requestCount = 0;
  private errorCount = 0;

  constructor() {
    this.initializeTemplates();
  }

  private initializeTemplates() {
    this.templates.set('lesson', {
      type: 'lesson',
      prompt: 'Create an engaging educational lesson about {topic} for {grade} students studying {subject}. The lesson should be {length} and written in a {style} style.',
      structure: ['introduction', 'main_content', 'examples', 'activities', 'summary', 'assessment']
    });

    this.templates.set('quiz', {
      type: 'quiz',
      prompt: 'Generate a comprehensive quiz about {topic} for {grade} students studying {subject}. Include multiple choice questions, true/false, and short answer questions.',
      structure: ['instructions', 'questions', 'answer_key', 'scoring']
    });

    this.templates.set('assignment', {
      type: 'assignment',
      prompt: 'Create an educational assignment about {topic} for {grade} students studying {subject}. The assignment should be {length} and encourage critical thinking.',
      structure: ['objective', 'instructions', 'requirements', 'rubric', 'submission_guidelines']
    });

    this.templates.set('summary', {
      type: 'summary',
      prompt: 'Write a clear and concise summary of {topic} for {grade} students studying {subject}. Focus on key concepts and main ideas.',
      structure: ['key_points', 'main_concepts', 'important_definitions', 'conclusion']
    });

    this.templates.set('explanation', {
      type: 'explanation',
      prompt: 'Provide a detailed explanation of {topic} for {grade} students studying {subject}. Use {style} language and include examples.',
      structure: ['concept_introduction', 'detailed_explanation', 'examples', 'common_misconceptions', 'practical_applications']
    });

    this.templates.set('story', {
      type: 'story',
      prompt: 'Create an educational story about {topic} for {grade} students studying {subject}. The story should be engaging and teach important concepts.',
      structure: ['setting', 'characters', 'plot', 'educational_elements', 'moral_lesson']
    });
  }

  async generateContent(request: ContentRequest): Promise<ContentResponse> {
    const startTime = Date.now();
    const contentId = this.generateContentId();
    
    try {
      // Verificar cache
      const cacheKey = this.generateCacheKey(request);
      const cachedContent = cache.get(cacheKey);
      
      if (cachedContent) {
        logger.info(`Cache hit for content generation: ${contentId}`);
        return {
          ...cachedContent,
          id: contentId,
          cached: true,
          timestamp: new Date()
        } as ContentResponse;
      }

      // Obtener template
      const template = this.templates.get(request.type);
      if (!template) {
        throw new Error(`Unsupported content type: ${request.type}`);
      }

      // Generar prompt
      const prompt = this.generatePrompt(template, request);

      // Generar contenido usando LLM
      const llmResponse = await this.callLLM(prompt);

      // Procesar y estructurar contenido
      const content = this.processContent(llmResponse, request, template);

      const response: ContentResponse = {
        id: contentId,
        type: request.type,
        title: content.title,
        content: content.content,
        metadata: content.metadata,
        processingTime: Date.now() - startTime,
        cached: false,
        timestamp: new Date()
      };

      // Guardar en cache
      cache.set(cacheKey, response);

      // Actualizar m√©tricas
      this.requestCount++;
      await this.updateMetrics(request.type, response.processingTime);

      logger.info(`Generated content: ${contentId}, type: ${request.type}, time: ${response.processingTime}ms`);
      
      return response;

    } catch (error) {
      this.errorCount++;
      logger.error(`Failed to generate content: ${contentId}`, error);
      throw error;
    }
  }

  private generatePrompt(template: ContentTemplate, request: ContentRequest): string {
    let prompt = template.prompt
      .replace('{topic}', request.topic)
      .replace('{subject}', request.subject)
      .replace('{grade}', request.grade)
      .replace('{style}', request.style)
      .replace('{length}', request.length);

    // Agregar instrucciones espec√≠ficas
    prompt += `\n\nPlease structure the content with the following sections: ${template.structure.join(', ')}.`;
    prompt += `\nWrite in ${request.language} language.`;
    prompt += `\nMake it appropriate for ${request.grade} level students.`;

    return prompt;
  }

  private async callLLM(prompt: string): Promise<string> {
    try {
      const response = await anthropic.messages.create({
        model: 'claude-3-sonnet-20240229',
        max_tokens: 4096,
        temperature: 0.7,
        messages: [{ role: 'user', content: prompt }]
      });

      return response.content[0].text;
    } catch (error) {
      logger.error('LLM call failed, trying Google Gemini as fallback', error);
      
      // Fallback a Google Gemini
      const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY || '');
      const model = genAI.getGenerativeModel({ model: 'gemini-pro' });
      
      const result = await model.generateContent(prompt);
      const response = await result.response;
      
      return response.text();
    }
  }

  private processContent(llmResponse: string, request: ContentRequest, template: ContentTemplate): any {
    // Procesar la respuesta del LLM y estructurarla
    const lines = llmResponse.split('\n');
    const title = lines[0].replace(/^#+\s*/, '') || `${request.topic} - ${request.type}`;
    
    // Extraer metadatos
    const metadata = {
      subject: request.subject,
      grade: request.grade,
      topic: request.topic,
      difficulty: this.estimateDifficulty(request.grade),
      estimatedTime: this.estimateTime(request.length),
      learningObjectives: this.extractLearningObjectives(llmResponse),
      keywords: this.extractKeywords(llmResponse, request.topic)
    };

    return {
      title,
      content: llmResponse,
      metadata
    };
  }

  private estimateDifficulty(grade: string): string {
    const gradeMap: Record<string, string> = {
      'K-2': 'beginner',
      '3-5': 'elementary',
      '6-8': 'intermediate',
      '9-12': 'advanced',
      'college': 'expert'
    };
    return gradeMap[grade] || 'intermediate';
  }

  private estimateTime(length: string): number {
    const timeMap: Record<string, number> = {
      'short': 15,
      'medium': 30,
      'long': 60
    };
    return timeMap[length] || 30;
  }

  private extractLearningObjectives(content: string): string[] {
    const objectives: string[] = [];
    const lines = content.split('\n');
    
    for (const line of lines) {
      if (line.toLowerCase().includes('objective') || line.toLowerCase().includes('goal')) {
        objectives.push(line.trim());
      }
    }
    
    return objectives.slice(0, 5); // M√°ximo 5 objetivos
  }

  private extractKeywords(content: string, topic: string): string[] {
    const keywords = [topic];
    const words = content.toLowerCase().split(/\s+/);
    const wordCount: Record<string, number> = {};
    
    for (const word of words) {
      if (word.length > 3 && !['the', 'and', 'for', 'with', 'this', 'that'].includes(word)) {
        wordCount[word] = (wordCount[word] || 0) + 1;
      }
    }
    
    const sortedWords = Object.entries(wordCount)
      .sort(([,a], [,b]) => b - a)
      .slice(0, 10)
      .map(([word]) => word);
    
    return [...new Set([...keywords, ...sortedWords])];
  }

  private generateContentId(): string {
    return `content_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private generateCacheKey(request: ContentRequest): string {
    return `content:${request.type}:${request.subject}:${request.grade}:${_.hash(request.topic)}`;
  }

  private async updateMetrics(contentType: string, processingTime: number): Promise<void> {
    const metricsKey = `content:metrics:${contentType}`;
    const metrics = await redisClient.get(metricsKey);
    
    const currentMetrics = metrics ? JSON.parse(metrics) : {
      requestCount: 0,
      totalProcessingTime: 0,
      averageProcessingTime: 0,
      lastUpdated: new Date().toISOString()
    };

    currentMetrics.requestCount++;
    currentMetrics.totalProcessingTime += processingTime;
    currentMetrics.averageProcessingTime = currentMetrics.totalProcessingTime / currentMetrics.requestCount;
    currentMetrics.lastUpdated = new Date().toISOString();

    await redisClient.setEx(metricsKey, 86400, JSON.stringify(currentMetrics));
  }

  async getHealthStatus(): Promise<Record<string, any>> {
    return {
      status: 'healthy',
      service: 'content-generation',
      templates: Array.from(this.templates.keys()),
      metrics: {
        requestCount: this.requestCount,
        errorCount: this.errorCount,
        errorRate: this.requestCount > 0 ? (this.errorCount / this.requestCount) * 100 : 0
      },
      cache: {
        size: cache.keys().length,
        hits: cache.getStats().hits,
        misses: cache.getStats().misses
      },
      timestamp: new Date().toISOString()
    };
  }

  async getContentTypes(): Promise<string[]> {
    return Array.from(this.templates.keys());
  }

  async clearCache(): Promise<void> {
    cache.flushAll();
    logger.info('Content Generation cache cleared');
  }
}

export default new ContentGenerationService();
"@

Set-Content -Path "microservices/content-generation/src/services/content-generation.ts" -Value $contentGenerationService

# 3. IMPLEMENTAR CHATBOT SERVICE
Write-Host "üí¨ Implementando Chatbot Service..." -ForegroundColor Yellow

$chatbotService = @"
import { createClient } from 'redis';
import winston from 'winston';
import { Anthropic } from '@anthropic-ai/sdk';
import NodeCache from 'node-cache';
import _ from 'lodash';

// Configurar logger
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// Configurar Redis
const redisClient = createClient({
  url: process.env.REDIS_URL || 'redis://localhost:6379'
});

// Configurar cache
const cache = new NodeCache({ stdTTL: 1800 }); // 30 minutos

// Configurar Anthropic
const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Interfaces
interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: Date;
}

interface ChatSession {
  id: string;
  userId: string;
  messages: ChatMessage[];
  context: Record<string, any>;
  createdAt: Date;
  updatedAt: Date;
  ttl: number;
}

interface ChatRequest {
  userId: string;
  sessionId: string;
  message: string;
  context?: Record<string, any>;
}

interface ChatResponse {
  id: string;
  sessionId: string;
  message: string;
  confidence: number;
  suggestedActions: string[];
  processingTime: number;
  timestamp: Date;
}

interface BotPersonality {
  name: string;
  description: string;
  systemPrompt: string;
  tone: string;
  expertise: string[];
}

class ChatbotService {
  private personalities: Map<string, BotPersonality> = new Map();
  private sessions: Map<string, ChatSession> = new Map();
  private requestCount = 0;
  private errorCount = 0;

  constructor() {
    this.initializePersonalities();
  }

  private initializePersonalities() {
    this.personalities.set('eduai-assistant', {
      name: 'EduAI Assistant',
      description: 'A helpful educational assistant for students and teachers',
      systemPrompt: `You are EduAI Assistant, a helpful educational AI designed to assist students and teachers. 
      You provide clear, accurate, and educational responses. You can help with:
      - Academic questions and explanations
      - Study tips and learning strategies
      - Homework assistance
      - Educational resources and recommendations
      - General knowledge questions
      
      Always be encouraging, patient, and educational in your responses.`,
      tone: 'friendly and educational',
      expertise: ['education', 'academic support', 'learning strategies', 'homework help']
    });

    this.personalities.set('math-tutor', {
      name: 'Math Tutor',
      description: 'Specialized math tutor for all levels',
      systemPrompt: `You are a specialized Math Tutor AI. You excel at:
      - Explaining mathematical concepts clearly
      - Providing step-by-step solutions
      - Helping with problem-solving strategies
      - Teaching mathematical thinking
      - Supporting students from basic to advanced math
      
      Always show your work and explain the reasoning behind each step.`,
      tone: 'patient and methodical',
      expertise: ['mathematics', 'problem solving', 'mathematical concepts', 'calculations']
    });

    this.personalities.set('science-mentor', {
      name: 'Science Mentor',
      description: 'Science expert for experiments and concepts',
      systemPrompt: `You are a Science Mentor AI. You specialize in:
      - Explaining scientific concepts and theories
      - Helping with science experiments and lab work
      - Teaching scientific method and critical thinking
      - Supporting biology, chemistry, physics, and earth science
      - Connecting science to real-world applications
      
      Encourage curiosity and scientific thinking.`,
      tone: 'curious and analytical',
      expertise: ['biology', 'chemistry', 'physics', 'earth science', 'scientific method']
    });

    this.personalities.set('language-coach', {
      name: 'Language Coach',
      description: 'Language learning and writing assistant',
      systemPrompt: `You are a Language Coach AI. You help with:
      - Grammar and writing improvement
      - Language learning and practice
      - Essay writing and composition
      - Reading comprehension
      - Vocabulary building
      - Multiple languages support
      
      Provide constructive feedback and encourage language development.`,
      tone: 'supportive and encouraging',
      expertise: ['grammar', 'writing', 'language learning', 'literature', 'communication']
    });
  }

  async processMessage(request: ChatRequest): Promise<ChatResponse> {
    const startTime = Date.now();
    const responseId = this.generateResponseId();
    
    try {
      // Obtener o crear sesi√≥n
      const session = await this.getOrCreateSession(request.userId, request.sessionId);
      
      // Agregar mensaje del usuario
      session.messages.push({
        role: 'user',
        content: request.message,
        timestamp: new Date()
      });

      // Actualizar contexto
      if (request.context) {
        session.context = { ...session.context, ...request.context };
      }

      // Determinar personalidad basada en el contexto
      const personality = this.determinePersonality(request.message, session.context);

      // Generar respuesta
      const response = await this.generateResponse(request.message, session, personality);

      // Agregar respuesta del asistente
      session.messages.push({
        role: 'assistant',
        content: response.message,
        timestamp: new Date()
      });

      // Actualizar sesi√≥n
      session.updatedAt = new Date();
      await this.saveSession(session);

      const chatResponse: ChatResponse = {
        id: responseId,
        sessionId: session.id,
        message: response.message,
        confidence: response.confidence,
        suggestedActions: response.suggestedActions,
        processingTime: Date.now() - startTime,
        timestamp: new Date()
      };

      // Actualizar m√©tricas
      this.requestCount++;
      await this.updateMetrics(personality.name, chatResponse.processingTime);

      logger.info(`Processed chat message: ${responseId}, session: ${session.id}, personality: ${personality.name}`);
      
      return chatResponse;

    } catch (error) {
      this.errorCount++;
      logger.error(`Failed to process chat message: ${responseId}`, error);
      throw error;
    }
  }

  private async getOrCreateSession(userId: string, sessionId: string): Promise<ChatSession> {
    const sessionKey = `chat:session:${userId}:${sessionId}`;
    const sessionData = await redisClient.get(sessionKey);
    
    if (sessionData) {
      const session: ChatSession = JSON.parse(sessionData);
      return session;
    }

    const newSession: ChatSession = {
      id: sessionId,
      userId,
      messages: [],
      context: {},
      createdAt: new Date(),
      updatedAt: new Date(),
      ttl: 3600 // 1 hora
    };

    await this.saveSession(newSession);
    return newSession;
  }

  private async saveSession(session: ChatSession): Promise<void> {
    const sessionKey = `chat:session:${session.userId}:${session.id}`;
    await redisClient.setEx(sessionKey, session.ttl, JSON.stringify(session));
  }

  private determinePersonality(message: string, context: Record<string, any>): BotPersonality {
    const messageLower = message.toLowerCase();
    
    // Detectar matem√°ticas
    if (messageLower.includes('math') || messageLower.includes('calculate') || 
        messageLower.includes('equation') || messageLower.includes('solve') ||
        /\d+[\+\-\*\/\=]/.test(message)) {
      return this.personalities.get('math-tutor')!;
    }
    
    // Detectar ciencia
    if (messageLower.includes('science') || messageLower.includes('experiment') ||
        messageLower.includes('biology') || messageLower.includes('chemistry') ||
        messageLower.includes('physics')) {
      return this.personalities.get('science-mentor')!;
    }
    
    // Detectar lenguaje
    if (messageLower.includes('grammar') || messageLower.includes('write') ||
        messageLower.includes('essay') || messageLower.includes('language') ||
        messageLower.includes('vocabulary')) {
      return this.personalities.get('language-coach')!;
    }
    
    // Por defecto, usar el asistente general
    return this.personalities.get('eduai-assistant')!;
  }

  private async generateResponse(message: string, session: ChatSession, personality: BotPersonality): Promise<any> {
    // Construir historial de conversaci√≥n
    const conversationHistory = session.messages
      .slice(-10) // √öltimos 10 mensajes
      .map(msg => `${msg.role}: ${msg.content}`)
      .join('\n');

    const prompt = `${personality.systemPrompt}

Conversation History:
${conversationHistory}

User: ${message}

Assistant:`;

    try {
      const response = await anthropic.messages.create({
        model: 'claude-3-sonnet-20240229',
        max_tokens: 1024,
        temperature: 0.7,
        messages: [{ role: 'user', content: prompt }]
      });

      const assistantMessage = response.content[0].text;

      // Analizar confianza y sugerir acciones
      const confidence = this.analyzeConfidence(message, assistantMessage);
      const suggestedActions = this.generateSuggestedActions(message, assistantMessage);

      return {
        message: assistantMessage,
        confidence,
        suggestedActions
      };

    } catch (error) {
      logger.error('Failed to generate response with Anthropic, using fallback', error);
      
      // Respuesta de fallback
      return {
        message: "I'm sorry, I'm having trouble processing your request right now. Please try again in a moment.",
        confidence: 0.3,
        suggestedActions: ['try_again', 'contact_support']
      };
    }
  }

  private analyzeConfidence(userMessage: string, assistantMessage: string): number {
    // An√°lisis simple de confianza basado en la respuesta
    let confidence = 0.7; // Base confidence
    
    // Aumentar confianza si la respuesta es larga y detallada
    if (assistantMessage.length > 100) confidence += 0.1;
    
    // Disminuir confianza si la respuesta es muy corta
    if (assistantMessage.length < 20) confidence -= 0.2;
    
    // Aumentar confianza si contiene explicaciones
    if (assistantMessage.includes('because') || assistantMessage.includes('explain')) confidence += 0.1;
    
    // Disminuir confianza si contiene disculpas
    if (assistantMessage.includes('sorry') || assistantMessage.includes('not sure')) confidence -= 0.2;
    
    return Math.max(0.1, Math.min(1.0, confidence));
  }

  private generateSuggestedActions(userMessage: string, assistantMessage: string): string[] {
    const actions: string[] = [];
    const messageLower = userMessage.toLowerCase();
    
    // Sugerir acciones basadas en el tipo de pregunta
    if (messageLower.includes('help') || messageLower.includes('how')) {
      actions.push('provide_example', 'show_steps', 'explain_concept');
    }
    
    if (messageLower.includes('practice') || messageLower.includes('exercise')) {
      actions.push('generate_practice', 'create_quiz', 'provide_exercises');
    }
    
    if (messageLower.includes('resource') || messageLower.includes('material')) {
      actions.push('recommend_resources', 'suggest_materials', 'provide_links');
    }
    
    // Acciones generales
    actions.push('continue_conversation', 'ask_follow_up');
    
    return actions.slice(0, 3); // M√°ximo 3 acciones
  }

  private generateResponseId(): string {
    return `chat_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private async updateMetrics(personality: string, processingTime: number): Promise<void> {
    const metricsKey = `chatbot:metrics:${personality}`;
    const metrics = await redisClient.get(metricsKey);
    
    const currentMetrics = metrics ? JSON.parse(metrics) : {
      requestCount: 0,
      totalProcessingTime: 0,
      averageProcessingTime: 0,
      lastUpdated: new Date().toISOString()
    };

    currentMetrics.requestCount++;
    currentMetrics.totalProcessingTime += processingTime;
    currentMetrics.averageProcessingTime = currentMetrics.totalProcessingTime / currentMetrics.requestCount;
    currentMetrics.lastUpdated = new Date().toISOString();

    await redisClient.setEx(metricsKey, 86400, JSON.stringify(currentMetrics));
  }

  async getHealthStatus(): Promise<Record<string, any>> {
    return {
      status: 'healthy',
      service: 'chatbot',
      personalities: Array.from(this.personalities.keys()),
      activeSessions: this.sessions.size,
      metrics: {
        requestCount: this.requestCount,
        errorCount: this.errorCount,
        errorRate: this.requestCount > 0 ? (this.errorCount / this.requestCount) * 100 : 0
      },
      cache: {
        size: cache.keys().length,
        hits: cache.getStats().hits,
        misses: cache.getStats().misses
      },
      timestamp: new Date().toISOString()
    };
  }

  async getPersonalities(): Promise<BotPersonality[]> {
    return Array.from(this.personalities.values());
  }

  async getSessionHistory(userId: string, sessionId: string): Promise<ChatMessage[]> {
    const session = await this.getOrCreateSession(userId, sessionId);
    return session.messages;
  }

  async clearSession(userId: string, sessionId: string): Promise<void> {
    const sessionKey = `chat:session:${userId}:${sessionId}`;
    await redisClient.del(sessionKey);
    logger.info(`Cleared chat session: ${userId}:${sessionId}`);
  }
}

export default new ChatbotService();
"@

Set-Content -Path "microservices/chatbot/src/services/chatbot.ts" -Value $chatbotService

Write-Host "‚úÖ FASE 2 - SERVICIOS AI IMPLEMENTADOS" -ForegroundColor Green
Write-Host ""
Write-Host "üìã Resumen de implementaci√≥n:" -ForegroundColor Cyan
Write-Host "‚úÖ LLM Gateway Service - Implementado" -ForegroundColor Green
Write-Host "‚úÖ Content Generation Service - Implementado" -ForegroundColor Green
Write-Host "‚úÖ Chatbot Service - Implementado" -ForegroundColor Green
Write-Host "‚úÖ Multi-provider LLM support (Anthropic, Google, OpenAI)" -ForegroundColor Green
Write-Host "‚úÖ Content templates y generaci√≥n inteligente" -ForegroundColor Green
Write-Host "‚úÖ Chatbot con personalidades especializadas" -ForegroundColor Green
Write-Host "‚úÖ Sistema de cache y m√©tricas" -ForegroundColor Green
Write-Host "‚úÖ Manejo de errores y fallbacks" -ForegroundColor Green
Write-Host ""
Write-Host "üöÄ Pr√≥ximos pasos:" -ForegroundColor Cyan
Write-Host "1. Continuar con FASE 3: Testing y Frontend" -ForegroundColor White
Write-Host "2. Implementar Predictive Analytics y Personalization Engine" -ForegroundColor White
Write-Host "3. Configurar integraci√≥n entre servicios" -ForegroundColor White
Write-Host ""
Write-Host "üéØ ¬°FASE 2 - SERVICIOS AI COMPLETADA!" -ForegroundColor Green 